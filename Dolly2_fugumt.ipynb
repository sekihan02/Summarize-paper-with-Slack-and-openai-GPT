{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8707ecdb-7745-4069-b98a-2058ef7b901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: slack_sdk==3.21.0 in /usr/local/lib/python3.8/dist-packages (3.21.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install slack_sdk==3.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8f9f0d-b5c9-4536-9b2c-64c2ca9653b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv==1.4.4 in /usr/local/lib/python3.8/dist-packages (1.4.4)\n",
      "Requirement already satisfied: feedparser in /usr/local/lib/python3.8/dist-packages (from arxiv==1.4.4) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.8/dist-packages (from feedparser->arxiv==1.4.4) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install arxiv==1.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f726c930-7060-42f2-9beb-e0e1d9c104ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers[torch]==4.25.1\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (0.13.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (4.65.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (2.28.2)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1) (2.0.0+cu117)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers[torch]==4.25.1) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (2.0.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (16.0.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (3.26.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]==4.25.1) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]==4.25.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers[torch]==4.25.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers[torch]==4.25.1) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch!=1.12.0,>=1.7->transformers[torch]==4.25.1) (1.3.0)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.28.1\n",
      "    Uninstalling transformers-4.28.1:\n",
      "      Successfully uninstalled transformers-4.28.1\n",
      "Successfully installed transformers-4.25.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install accelerate>=0.12.0\n",
    "!pip3 install transformers[torch]==4.25.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9f605-7fd7-4353-a052-a8c679f259d6",
   "metadata": {},
   "source": [
    "# SlackとDolly 2.0のGPTで論文の要約をする\n",
    "\n",
    "## Reference\n",
    "- [DataBricks Dolly 2.0, Commercial Use, TRUE Open Source LLM](https://www.youtube.com/watch?v=GpWqjNf0SCM)\n",
    "- [最新の論文をChatGPTで要約して毎朝Slackに共有してくれるbotを作る！](https://zenn.dev/ozushi/articles/ebe3f47bf50a86)\n",
    "- [Slack API を使用してメッセージを投稿する](https://zenn.dev/kou_pg_0131/articles/slack-api-post-message)\n",
    "- [【Slack】インストールするボットユーザーがありませんと出たときの対処方法](https://the-simple.jp/slack-nobotuser#Step1Bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53335cc1-4265-458a-9530-c3330f5654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a128ba8e-74b4-4299-9fb9-c1d3dd3ebe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 03:02:22.457424: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1f4fcb-d175-41f2-b095-562ac3791fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802033a1-253d-4ac0-8619-c98b0908f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLACK_API_TOKEN = 'SLACK_API_TOKEN'    # ボットとして API を実行するためのトークン\n",
    "# Slackに投稿するチャンネル名を指定する\n",
    "SLACK_CHANNEL = \"要約\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56bf1a3e-98b8-420e-a564-299c2e6eab2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1929bac742364e9a82bd91e89300cc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d48efc4904428596cf681e9c05038e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)instruct_pipeline.py:   0%|          | 0.00/7.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c81aa5c427c4d49a4e36eb08b633877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c315d63c2fc43b493da54fdadbac4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165f5a959fe34f97a7a3fd834723565f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2de2a061ffc4968ac8e4f0bbc1e6917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text = pipeline(model=\"databricks/dolly-v2-2-8b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map='auto' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ebe6d8-3518-49cc-9529-616c5f189812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(result):\n",
    "    system = \"\"\"与えられた論文の全体を128文字以内にまとめた後、タイトルと原文とその日本語訳、概要、日付、新規性や差分、独特の手法、実験結果（評価結果の数値がある場合は、この実験結果にその内容を書いてください）を以下のフォーマットで日本語で出力してください。```\n",
    "    # タイトルの原文\n",
    "    # タイトルの日本語訳\n",
    "    # リンク\n",
    "    # 日付(yyyy/MM/dd)\n",
    "    ## 一言でいうと\n",
    "    ### 概要\n",
    "    ### 新規性・差分\n",
    "    ### 手法\n",
    "    ### 結果\n",
    "    ### コメント\n",
    "    ```\"\"\"\n",
    "\n",
    "    system_eng = \"\"\"After summarizing the entire given paper in 128 characters or less, please output the title, original text and its Japanese translation, summary, date, novelty or difference, unique methods, and experimental results (if there are numerical evaluation results, please describe them in this experimental results) in Japanese in the following format. ````\n",
    "    # Original title\n",
    "    # Japanese translation of title\n",
    "    # Link\n",
    "    # Date (yyyy/MM/dd)\n",
    "    ## In a nutshell.\n",
    "    ### Summary\n",
    "    ### Novelty/Difference\n",
    "    ### Methodology\n",
    "    ### Results\n",
    "    ### Comments\n",
    "    ```\"\"\"\n",
    "    \n",
    "    # 論文の要約を取得して日本語に翻訳する\n",
    "    summary = result.summary\n",
    "    # 論文のタイトルを取得して日本語に翻訳する\n",
    "    title = result.title\n",
    "    \n",
    "    text = f\"title: {title}\\nbody: {summary}\"\n",
    "    date_str = result.published.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'# 日付(yyyy/MM/dd)\\n{date_str}')\n",
    "    print(text)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Load the MarianMTModel and MarianTokenizer for English to Japanese translation\n",
    "    fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\n",
    "    \n",
    "    try:\n",
    "        # j_text = fugu_translator(text)\n",
    "        \n",
    "        # Dolly2.0で要約して翻訳の実施\n",
    "        eng_dooly2_generate_text = generate_text(text)\n",
    "        j_text = fugu_translator(eng_dooly2_generate_text)\n",
    "    except IndexError:\n",
    "        j_text = []\n",
    "        pass\n",
    "    \n",
    "    japanese_translations = []\n",
    "    for translation in j_text:\n",
    "        japanese_translations.append(translation['translation_text'])\n",
    "    \n",
    "    japanese_text = ''.join(japanese_translations)\n",
    "    print(japanese_text)\n",
    "    \n",
    "    text_ = f\"title: {title}\\ndate: {date_str}\\n\"\n",
    "    japan_text = text_ + japanese_text\n",
    "    print()\n",
    "    return japan_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0af268-b7f4-4dfc-9b94-2ac535d31f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryを用意\n",
    "# query_list = ['ti:%22 Anomaly Detection %22', 'ti:%22 AIOps %22']\n",
    "query_list = ['AIOps', 'Anomaly Detection', 'Ops']\n",
    "message_list = ['AIOps', 'Anomaly Detection', 'Ops']\n",
    "\n",
    "# query_list = ['AIOps']\n",
    "# message_list = ['AIOps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a535352-2b11-49d8-ae70-d2093712c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slack APIクライアントを初期化する\n",
    "client = WebClient(token=SLACK_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aafaf00-afb4-4ce4-83cf-868143c3c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2212.13245v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2022-12-26 18:24:45\n",
      "title: Studying the Characteristics of AIOps Projects on GitHub\n",
      "body: Artificial Intelligence for IT Operations (AIOps) leverages AI approaches to\n",
      "handle the massive data generated during the operations of software systems.\n",
      "Prior works have proposed various AIOps solutions to support different tasks in\n",
      "system operations and maintenance (e.g., anomaly detection). In this work, we\n",
      "investigate open-source AIOps projects in-depth to understand the\n",
      "characteristics of AIOps in practice. We first carefully identify a set of\n",
      "AIOps projects from GitHub and analyze their repository metrics (e.g., the used\n",
      "programming languages). Then, we qualitatively study the projects to understand\n",
      "their input data, analysis techniques, and goals. Finally, we analyze the\n",
      "quality of these projects using different quality metrics, such as the number\n",
      "of bugs. We also sample two sets of baseline projects from GitHub: a random\n",
      "sample of machine learning projects, and a random sample of general purpose\n",
      "projects. We compare different metrics of our identified AIOps projects with\n",
      "these baselines. Our results show a recent and growing interest in AIOps\n",
      "solutions. However, the quality metrics indicate that AIOps projects suffer\n",
      "from more issues than our baseline projects. We also pinpoint the most common\n",
      "issues in AIOps approaches and discuss the possible solutions to overcome them.\n",
      "Our findings help practitioners and researchers understand the current state of\n",
      "AIOps practices and sheds light to different ways to improve AIOps weak\n",
      "aspects. To the best of our knowledge, this work is the first to characterize\n",
      "open source AIOps projects.\n",
      "\n",
      "タイトル: GitHubでAIOpsプロジェクトの特性を研究する: ITオペレーション(AIOps)のための人工知能(Artificial Intelligence for IT Operations)は、ソフトウェアシステムの運用中に生成された大量のデータを処理するためにAIアプローチを活用する。以前の研究は、システム操作とメンテナンス(異常検出など)において、さまざまなタスクをサポートするAIOpsソリューションを提案してきた。この研究では、AIOpsの特性を理解するために、オープンソースのAIOpsプロジェクトを詳細に調査する。まず、GitHubからAIOpsプロジェクトのセットを慎重に識別し、リポジトリメトリクス(例えば、使用されるプログラミング言語)を分析する。その後、入力データ、分析技術、目標を理解するためのプロジェクトを定性的に研究する。最後に、バグの数などの異なる品質メトリクスを使用して、これらのプロジェクトの品質を分析する。また、GitHubから2つのベースラインプロジェクトをサンプリングした: 機械学習プロジェクトのランダムなサンプルと、一般的な目的のランダムなサンプル。\n",
      "\n",
      "Message posted: 1681618297.870249\n",
      "http://arxiv.org/abs/2301.08851v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-01-21 01:58:34\n",
      "title: LWS: A Framework for Log-based Workload Simulation in Session-based SUT\n",
      "body: Microservice-based applications and cloud-native systems have been widely\n",
      "applied in large IT enterprises. The operation and management of\n",
      "microservice-based applications and cloud-native systems have become the focus\n",
      "of research. Essential and real workloads are the premise and basis of\n",
      "prominent research topics including performance testing, dynamic resource\n",
      "provisioning and scheduling, and AIOps. Due to the privacy restriction, the\n",
      "complexity and variety of workloads, and the requirements for reasonable\n",
      "intervention, it is difficult to copy or generate real workloads directly. In\n",
      "this paper, we formulate the task of workload simulation and propose a\n",
      "framework for Log-based Workload Simulation (LWS) in session-based application\n",
      "systems. First, LWS collects session logs and transforms them into grouped and\n",
      "well-organized sessions. Then LWS extracts the user behavior abstraction based\n",
      "on a relational model and the intervenable workload intensity by three methods\n",
      "from different perspectives. LWS combines the user behavior abstraction and the\n",
      "workload intensity for simulated workload generation and designs a\n",
      "domain-specific language for better execution. The experimental evaluation is\n",
      "performed on an open-source cloud-native application and a public real-world\n",
      "e-commerce workload. The experimental results show that the simulated workload\n",
      "generated by LWS is effective and intervenable.\n",
      "\n",
      "タイトル: Log-based Workload Simulation in Session-based Application Systems body: マイクロサービスベースのアプリケーションとクラウドネイティブシステムは、大規模IT企業に広く適用されている。マイクロサービスベースのアプリケーションとクラウドネイティブシステムの運用と管理は、研究の焦点となっている。本質的および実際のワークロードは、パフォーマンステスト、動的リソースプロビジョニングとスケジューリング、AIOpsなどの顕著な研究トピックの前提と基礎である。プライバシー制限、複雑さとワークロードの多様性、合理的な介入の要件により、実際のワークロードを直接コピーまたは生成することは困難である。本稿では、ワークロードシミュレーションのタスクを定式化し、セッションベースのアプリケーションシステムにおけるLog-based Workload Simulation(LWS)のフレームワークを提案する。まず、LWSはセッションログを収集し、それらをグループ化およびよく組織化されたセッションに変換する。LWSは、ユーザの行動の抽象化を、実際のワークロードを直接コピーまたは生成するのは難しい。\n",
      "\n",
      "Message posted: 1681618429.508519\n",
      "http://arxiv.org/abs/2304.04661v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-10 15:38:12\n",
      "title: AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges\n",
      "body: Artificial Intelligence for IT operations (AIOps) aims to combine the power\n",
      "of AI with the big data generated by IT Operations processes, particularly in\n",
      "cloud infrastructures, to provide actionable insights with the primary goal of\n",
      "maximizing availability. There are a wide variety of problems to address, and\n",
      "multiple use-cases, where AI capabilities can be leveraged to enhance\n",
      "operational efficiency. Here we provide a review of the AIOps vision, trends\n",
      "challenges and opportunities, specifically focusing on the underlying AI\n",
      "techniques. We discuss in depth the key types of data emitted by IT Operations\n",
      "activities, the scale and challenges in analyzing them, and where they can be\n",
      "helpful. We categorize the key AIOps tasks as - incident detection, failure\n",
      "prediction, root cause analysis and automated actions. We discuss the problem\n",
      "formulation for each task, and then present a taxonomy of techniques to solve\n",
      "these problems. We also identify relatively under explored topics, especially\n",
      "those that could significantly benefit from advances in AI literature. We also\n",
      "provide insights into the trends in this field, and what are the key investment\n",
      "opportunities.\n",
      "\n",
      "タイトル:AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges body: Artificial Intelligence for IT Operations (AIOps)は、AIの力を、特にクラウドインフラストラクチャにおいて、IT運用プロセスによって生成されたビッグデータと組み合わせて、可用性を最大化するための行動可能な洞察を提供することを目的としている。AIの能力を運用効率を高めるために活用できるさまざまな問題と、複数のユースケースがある。ここでは、AIOpsのビジョン、傾向の課題と機会のレビューを提供する。我々は、IT運用活動によって発生するデータの種類、分析の規模と課題、それらがどこに役立つかについて詳細に論じる。重要なAIOpsタスクを、インシデント検出、障害予測、根本原因分析、自動化されたアクションとして分類する。これらの課題についても、これらの課題を詳細に検討し、これらの課題を解決し、また、これらの課題の課題を解決するために、これらの課題を議論する。\n",
      "\n",
      "Message posted: 1681618557.675159\n",
      "http://arxiv.org/abs/2211.15739v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2022-11-28 19:41:56\n",
      "title: CWD: A Machine Learning based Approach to Detect Unknown Cloud Workloads\n",
      "body: Workloads in modern cloud data centers are becoming increasingly complex. The\n",
      "number of workloads running in cloud data centers has been growing\n",
      "exponentially for the last few years, and cloud service providers (CSP) have\n",
      "been supporting on-demand services in real-time. Realizing the growing\n",
      "complexity of cloud environment and cloud workloads, hardware vendors such as\n",
      "Intel and AMD are increasingly introducing cloud-specific workload acceleration\n",
      "features in their CPU platforms. These features are typically targeted towards\n",
      "popular and commonly-used cloud workloads. Nonetheless, uncommon,\n",
      "customer-specific workloads (unknown workloads), if their characteristics are\n",
      "different from common workloads (known workloads), may not realize the\n",
      "potential of the underlying platform. To address this problem of realizing the\n",
      "full potential of the underlying platform, we develop a machine learning based\n",
      "technique to characterize, profile and predict workloads running in the cloud\n",
      "environment. Experimental evaluation of our technique demonstrates good\n",
      "prediction performance. We also develop techniques to analyze the performance\n",
      "of the model in a standalone manner.\n",
      "\n",
      "タイトル: CWD: 未知のクラウドワークロードを検出するための機械学習ベースのアプローチ: 現代のクラウドデータセンタのワークロードはますます複雑化しています。過去数年間、クラウドデータセンターで実行されるワークロードの数は指数関数的に増加しており、クラウドサービスプロバイダ(CSP)はオンデマンドサービスをリアルタイムでサポートしています。クラウド環境やクラウドワークロードの複雑さの増大を実現するため、IntelやAMDなどのハードウェアベンダは、CPUプラットフォームにクラウド固有のワークロードアクセラレーション機能を導入しています。これらの機能は、一般的には一般的なクラウドワークロードを対象としていますが、一般的なワークロード(既知のワークロード)とは異なる場合、その特性は基盤となるプラットフォームの可能性に気付かないかもしれません。我々は、機械学習ベースのテクニックの全体的可能性を実現するために、機械学習ベースのパフォーマンス予測、予測、予測、予測、予測、予測、予測、予測、予測、予測、予測、予測、予測などの方法も提供しています。\n",
      "\n",
      "Message posted: 1681618674.098929\n",
      "http://arxiv.org/abs/2304.02829v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-06 02:34:07\n",
      "title: SoK: Machine Learning for Continuous Integration\n",
      "body: Continuous Integration (CI) has become a well-established software\n",
      "development practice for automatically and continuously integrating code\n",
      "changes during software development. An increasing number of Machine Learning\n",
      "(ML) based approaches for automation of CI phases are being reported in the\n",
      "literature. It is timely and relevant to provide a Systemization of Knowledge\n",
      "(SoK) of ML-based approaches for CI phases. This paper reports an SoK of\n",
      "different aspects of the use of ML for CI. Our systematic analysis also\n",
      "highlights the deficiencies of the existing ML-based solutions that can be\n",
      "improved for advancing the state-of-the-art.\n",
      "\n",
      "タイトル: Systemization of Knowledge (SoK): Machine Learning for Continuous Integration body: Continuous Integration (CI)は、ソフトウェア開発中のコード変更を自動的かつ継続的に統合するための、確立されたソフトウェア開発プラクティスとなっている。CIフェーズの自動化のための機械学習(ML)ベースのアプローチの増加が文献で報告されている。CIフェーズに対するMLベースのアプローチのシステム化(SoK)を提供することは、タイムリーかつ関連性がある。本稿では、CIに対するMLの使用の異なる側面のSoKについて報告する。\n",
      "\n",
      "Message posted: 1681618728.459889\n",
      "http://arxiv.org/abs/2304.06653v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 16:28:07\n",
      "title: G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection\n",
      "body: It has been reported that clustering-based topic models, which cluster\n",
      "high-quality sentence embeddings with an appropriate word selection method, can\n",
      "generate better topics than generative probabilistic topic models. However,\n",
      "these approaches suffer from the inability to select appropriate parameters and\n",
      "incomplete models that overlook the quantitative relation between words with\n",
      "topics and topics with text. To solve these issues, we propose graph to topic\n",
      "(G2T), a simple but effective framework for topic modelling. The framework is\n",
      "composed of four modules. First, document representation is acquired using\n",
      "pretrained language models. Second, a semantic graph is constructed according\n",
      "to the similarity between document representations. Third, communities in\n",
      "document semantic graphs are identified, and the relationship between topics\n",
      "and documents is quantified accordingly. Fourth, the word--topic distribution\n",
      "is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T\n",
      "achieved state-of-the-art performance on both English and Chinese documents\n",
      "with different lengths. Human judgements demonstrate that G2T can produce\n",
      "topics with better interpretability and coverage than baselines. In addition,\n",
      "G2T can not only determine the topic number automatically but also give the\n",
      "probabilistic distribution of words in topics and topics in documents. Finally,\n",
      "G2T is publicly available, and the distillation experiments provide instruction\n",
      "on how it works.\n",
      "\n",
      "グラフ・トゥ・トピック(G2T)は、トピックモデリングのための単純だが効果的なフレームワークである。G2Tは、4つのモジュールで構成されている。1. 文書表現は、事前訓練された言語モデルを用いて取得される: 文 -- 文書行列は、まずグラフとして表現される。そして、事前訓練された言語モデルを用いて、文書行列がコミュニティとして表現される。この方法で、トピックとの単語関係が定量化される。2. 文書表現の類似性に応じて意味グラフを構築する: 言語モデルの事前訓練されたパラメータとテキストを用いて、各文書に意味グラフを構築する。次に、意味的類似度測定を用いて、類似度を分類する。文書意味グラフのコミュニティが識別される: 意味的グラフのコミュニティは、様々な意味グラフカーネルを用いて、トピックと文書の関係が定量化されるk平均クラスタリングに基づいており、その意味的グラフカーネルを用いて意味的分布が計算される。\n",
      "\n",
      "Message posted: 1681618864.681829\n",
      "http://arxiv.org/abs/2304.06710v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 17:57:54\n",
      "title: Remote Sensing Change Detection With Transformers Trained from Scratch\n",
      "body: Current transformer-based change detection (CD) approaches either employ a\n",
      "pre-trained model trained on large-scale image classification ImageNet dataset\n",
      "or rely on first pre-training on another CD dataset and then fine-tuning on the\n",
      "target benchmark. This current strategy is driven by the fact that transformers\n",
      "typically require a large amount of training data to learn inductive biases,\n",
      "which is insufficient in standard CD datasets due to their small size. We\n",
      "develop an end-to-end CD approach with transformers that is trained from\n",
      "scratch and yet achieves state-of-the-art performance on four public\n",
      "benchmarks. Instead of using conventional self-attention that struggles to\n",
      "capture inductive biases when trained from scratch, our architecture utilizes a\n",
      "shuffled sparse-attention operation that focuses on selected sparse informative\n",
      "regions to capture the inherent characteristics of the CD data. Moreover, we\n",
      "introduce a change-enhanced feature fusion (CEFF) module to fuse the features\n",
      "from input image pairs by performing a per-channel re-weighting. Our CEFF\n",
      "module aids in enhancing the relevant semantic changes while suppressing the\n",
      "noisy ones. Extensive experiments on four CD datasets reveal the merits of the\n",
      "proposed contributions, achieving gains as high as 14.27\\% in\n",
      "intersection-over-union (IoU) score, compared to the best-published results in\n",
      "the literature. Code is available at\n",
      "\\url{https://github.com/mustansarfiaz/ScratchFormer}.\n",
      "\n",
      "タイトル: ScratchFormer: Scratchbodyからトレーニングされたトランスフォーマーを用いたエンドツーエンドリモートセンシング変更検出: 現在のトランスフォーマーベースの変更検出(CD)アプローチは、大規模な画像分類ImageNetデータセットでトレーニングされた事前トレーニングモデルを採用するか、別のCDデータセットでトレーニングされた最初の事前トレーニングに依存し、ターゲットベンチマークを微調整する。この現在の戦略は、トランスフォーマーが通常、小さなサイズで標準CDデータセットでは不十分な帰納バイアスを学習するために大量のトレーニングデータを必要とするという事実によって駆動される。我々は、スクラッチからトレーニングされたトランスフォーマーによるエンドツーエンドCDアプローチを開発し、4つの公開ベンチマークで最先端のパフォーマンスを達成する。従来の自己認識を使用する代わりに、スクラッチからトレーニングされた帰納バイアスをキャプチャするときに苦労する従来の自己認識を使用する代わりに、我々のアーキテクチャは、選択された操作特性に焦点をあてたインフォメーション機能を提供する。\n",
      "\n",
      "Message posted: 1681618999.225109\n",
      "http://arxiv.org/abs/2304.06696v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 17:50:15\n",
      "title: Improving novelty detection with generative adversarial networks on hand gesture data\n",
      "body: We propose a novel way of solving the issue of classification of\n",
      "out-of-vocabulary gestures using Artificial Neural Networks (ANNs) trained in\n",
      "the Generative Adversarial Network (GAN) framework. A generative model augments\n",
      "the data set in an online fashion with new samples and stochastic target\n",
      "vectors, while a discriminative model determines the class of the samples. The\n",
      "approach was evaluated on the UC2017 SG and UC2018 DualMyo data sets. The\n",
      "generative models performance was measured with a distance metric between\n",
      "generated and real samples. The discriminative models were evaluated by their\n",
      "accuracy on trained and novel classes. In terms of sample generation quality,\n",
      "the GAN is significantly better than a random distribution (noise) in mean\n",
      "distance, for all classes. In the classification tests, the baseline neural\n",
      "network was not capable of identifying untrained gestures. When the proposed\n",
      "methodology was implemented, we found that there is a trade-off between the\n",
      "detection of trained and untrained gestures, with some trained samples being\n",
      "mistaken as novelty. Nevertheless, a novelty detection accuracy of 95.4% or\n",
      "90.2% (depending on the data set) was achieved with just 5% loss of accuracy on\n",
      "trained classes.\n",
      "\n",
      "タイトル: ハンドジェスチャデータ本体上の生成的逆ネットワークを用いた語彙外ジェスチャの分類の新たな解法: GAN(Generative Adversarial Network)フレームワークでトレーニングされた人工ニューラルネットワーク(ANN)を用いた語彙外ジェスチャの分類の問題を解決する新しい方法を提案する。生成モデルは新しいサンプルと確率的ターゲットベクトルでオンライン形式でデータセットを増強し、識別モデルはサンプルのクラスを決定する。このアプローチはUC2017 SGとUC2018デュアルミオデータセットで評価された。生成モデルの性能は、生成されたサンプルと実際のサンプルの間の距離メトリックで測定された。識別モデルは、トレーニングされたクラスと新しいクラスの精度によって評価された。サンプル生成では、GANはランダムな分布(ノイズ)よりも大幅に優れており、ニューラルネットワークの分類には適していない。\n",
      "\n",
      "Message posted: 1681619126.349239\n",
      "http://arxiv.org/abs/2304.06640v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 16:09:25\n",
      "title: Prospects for detecting anisotropies and polarization of the stochastic gravitational wave background with ground-based detectors\n",
      "body: We build an analytical framework to study the observability of anisotropies\n",
      "and a net chiral polarization of the Stochastic Gravitational Wave Background\n",
      "(SGWB) with a generic network of ground-based detectors. We apply this\n",
      "formalism to perform a Fisher forecast of the performance of a network\n",
      "consisting of the current interferometers (LIGO, Virgo and KAGRA) and planned\n",
      "third-generation ones, such as the Einstein Telescope and Cosmic Explorer. Our\n",
      "results yield limits on the observability of anisotropic modes, spanning across\n",
      "noise- and signal-dominated regimes. We find that if the isotropic component of\n",
      "the SGWB has an amplitude close to the current limit, third-generation\n",
      "interferometers with an observation time of $10$ years can measure multipoles\n",
      "(in a spherical harmonic expansion) up to $\\ell = 8$ with ${\\cal O }\\left(\n",
      "10^{-3} - 10^{-2} \\right)$ accuracy relative to the isotropic component, and an\n",
      "${\\cal O }\\left( 10^{-3} \\right)$ amount of net polarization. For weaker\n",
      "signals, the accuracy worsens as roughly the inverse of the SGWB amplitude.\n",
      "\n",
      "地中検出器によるSGWBの異方性検出と偏光化の展望\n",
      "\n",
      "Message posted: 1681619136.407199\n",
      "http://arxiv.org/abs/2304.06693v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 17:49:15\n",
      "title: CATS: The Hubble Constant from Standardized TRGB and Type Ia Supernova Measurements\n",
      "body: The Tip of the Red Giant Branch (TRGB) provides a luminous standard candle\n",
      "for constructing distance ladders to measure the Hubble constant. In practice\n",
      "its measurements via edge-detection response (EDR) are complicated by the\n",
      "apparent fuzziness of the tip and the multi-peak landscape of the EDR. As a\n",
      "result, it can be difficult to replicate due to a case-by-case measurement\n",
      "process. Previously we optimized an unsupervised algorithm, Comparative\n",
      "Analysis of TRGBs (CATs), to minimize the variance among multiple halo fields\n",
      "per host without reliance on individualized choices, achieving state-of-the-art\n",
      "$\\sim$ $<$ 0.05 mag distance measures for optimal data. Further, we found an\n",
      "empirical correlation at 5$\\sigma$ confidence in the GHOSTS halo survey between\n",
      "our measurements of the tip and their contrast ratios (ratio of stars 0.5 mag\n",
      "just below and above the tip), useful for standardizing the apparent tips at\n",
      "different host locations. Here, we apply this algorithm to an expanded sample\n",
      "of SN Ia hosts to standardize these to multiple fields in the geometric anchor,\n",
      "NGC 4258. In concert with the Pantheon$+$ SN Ia sample, this analysis produces\n",
      "a (baseline) result of $H_0= 73.22 \\pm 2.06$ km/s/Mpc. The largest difference\n",
      "in $H_0$ between this and similar studies employing the TRGB derives from\n",
      "corrections for SN survey differences and local flows used in most recent SN Ia\n",
      "compilations but which were absent in earlier studies. SN-related differences\n",
      "total $\\sim$ 2.0 km/s/Mpc. A smaller share, $\\sim$ 1.4 km/s/Mpc, results from\n",
      "the inhomogeneity of the TRGB calibration across the distance ladder. We employ\n",
      "a grid of 108 variants around the optimal TRGB algorithm and find the median of\n",
      "variants is $72.94\\pm1.98$ km/s/Mpc with an additional uncertainty due to\n",
      "algorithm choices of 0.83 km/s/Mpc. None of these TRGB variants result in $H_0$\n",
      "less than 71.6 km/s/Mpc.\n",
      "\n",
      "タイトル: CATS: The Hubble Constant from Standardized TRGB and Type Ia Supernova Measurements body: The Tip of the Red Giant Branch (TRGB) は、ハッブル定数を測定するために距離子を構築するための光標準キャンドルを提供する。実際、エッジ検出応答(EDR)による測定は、エッジ検出応答(EDR)による明らかなファジリティとEDRのマルチピークランドスケープによって複雑である。結果として、ケースバイケースの測定プロセスによる再現が困難である可能性がある。以前は、教師なしのアルゴリズムであるTRGBの比較分析(CAT)を最適化し、個々の選択に依存せずにホストあたりの複数のハローフィールド間のばらつきを最小限に抑え、最適なデータに対する最先端の$\\sim$0.05マグ距離測定を達成した。さらに、最適なデータに対する実証的相関が得られた。\n",
      "\n",
      "Message posted: 1681619322.781679\n",
      "http://arxiv.org/abs/2304.05148v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-11 11:29:51\n",
      "title: High-performance and Scalable Software-based NVMe Virtualization Mechanism with I/O Queues Passthrough\n",
      "body: NVMe(Non-Volatile Memory Express) is an industry standard for solid-state\n",
      "drives (SSDs) that has been widely adopted in data centers. NVMe virtualization\n",
      "is crucial in cloud computing as it allows for virtualized NVMe devices to be\n",
      "used by virtual machines (VMs), thereby improving the utilization of storage\n",
      "resources. However, traditional software-based solutions have flexibility\n",
      "benefits but often come at the cost of performance degradation or high CPU\n",
      "overhead. On the other hand, hardware-assisted solutions offer high performance\n",
      "and low CPU usage, but their adoption is often limited by the need for special\n",
      "hardware support or the requirement for new hardware development.\n",
      "  In this paper, we propose LightIOV, a novel software-based NVMe\n",
      "virtualization mechanism that achieves high performance and scalability without\n",
      "consuming valuable CPU resources and without requiring special hardware\n",
      "support. LightIOV can support thousands of VMs on each server. The key idea\n",
      "behind LightIOV is NVMe hardware I/O queues passthrough, which enables VMs to\n",
      "directly access I/O queues of NVMe devices, thus eliminating virtualization\n",
      "overhead and providing near-native performance. Results from our experiments\n",
      "show that LightIOV can provide comparable performance to VFIO, with an IOPS of\n",
      "97.6%-100.2% of VFIO. Furthermore, in high-density VMs environments, LightIOV\n",
      "achieves 31.4% lower latency than SPDK-Vhost when running 200 VMs, and an\n",
      "improvement of 27.1% in OPS performance in real-world applications.\n",
      "\n",
      "タイトル: 高性能でスケーラブルなソフトウェアベースのI/OキューによるNVMe仮想化メカニズムパススルーボディ: NVMe(Non-Volatile Memory Express)は、データセンターで広く採用されているソリッドステートドライブ(SSD)の業界標準である。NVMe仮想化は、仮想マシン(VM)で仮想化されたNVMeデバイスの使用を可能にするため、ストレージリソースの利用を改善するため、クラウドコンピューティングにおいて重要である。しかし、従来のソフトウェアベースのソリューションには柔軟性の利点があるが、パフォーマンス劣化やCPUオーバーヘッドが高いコストがかかることが多い。一方で、ハードウェア支援ソリューションは高いパフォーマンスと低いCPU使用率を提供するが、特別なハードウェアサポートや新しいハードウェア開発の必要性によって制限されることが多い。本稿では、高いパフォーマンスとスケーラビリティを実現する新しいソフトウェアベースのNVMe仮想化メカニズムであるLightIOVを提案する。\n",
      "\n",
      "Message posted: 1681619486.215699\n",
      "http://arxiv.org/abs/2304.05599v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-12 04:16:40\n",
      "title: Bit-Interleaved Multiple Access: Improved Fairness, Reliability, and Latency for Massive IoT Networks\n",
      "body: In this paper, we propose bit-interleaved multiple access (BIMA) to enable\n",
      "Internet-of-Things (IoT) networks where a massive connection is required with\n",
      "limited resource blocks. First, by providing a true power allocation (PA)\n",
      "constraint for conventional NOMA with practical constraints, we demonstrate\n",
      "that it cannot support massive connections. To this end, we propose BIMA where\n",
      "there are no strict PA constraints, unlike conventional NOMA, thus allowing a\n",
      "high number of devices. We provide a comprehensive analytical framework for\n",
      "BIMA for all key performance indicators (KPIs) (i.e., ergodic capacity [EC],\n",
      "outage probability [OP], and bit error rate [BER]). We evaluate Jain's fairness\n",
      "index and proportional fairness index in terms of all KPIs. Based on the\n",
      "extensive computer simulations, we reveal that BIMA outperforms conventional\n",
      "NOMA significantly, with a performance gain of up to 20-30dB. This performance\n",
      "gain becomes greater when more devices are supported. BIMA provides a full\n",
      "diversity order and enables the implementation of an arbitrary number of\n",
      "devices and modulation orders, which is crucial for IoT networks in dense\n",
      "areas. BIMA guarantees a fairness system where none of the devices gets a\n",
      "severe performance and the sum-rate is shared in a fair manner among devices by\n",
      "guarantying QoS satisfaction. Finally, we provide an intense complexity and\n",
      "latency analysis and demonstrate that BIMA provides lower latency compared to\n",
      "conventional NOMA since it allows parallel computing at the receivers and no\n",
      "iterative operations are required. We show that BIMA reduces latency by up to\n",
      "350\\% for specific devices and 170\\% on average.\n",
      "\n",
      "タイトル: Bit-Interleaved Multiple Access (BIMA): 改善されたフェアネス、信頼性、および大規模IoTネットワークのレイテンシ - 'Chetan Ventresca, , Pankaj Vaid, , Rogerio Mohan, and Asim Hassan, [^1] [^2]' 書誌: - 'citation.bib' タイトル: 'Bit-Interleaved Multiple Access (BIMA): 改善されたフェアネス、信頼性、および大規模IoTネットワークのレイテンシ -- インターネット(IoT)、大規模なIoT、NOMA、BIMA、レイテンシ、複雑性、フェアネス - 導入===============近年では、モノのインターネット(IoT)は、インターネット上の膨大な数の物理オブジェクトの監視と制御を可能にする有望な技術として登場しています。\n",
      "\n",
      "Message posted: 1681619671.660759\n",
      "http://arxiv.org/abs/2304.02696v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-05 18:45:01\n",
      "title: Carrollian Approach to $1+3$D Flat Holography\n",
      "body: The isomorphism between the (extended) BMS$_4$ algebra and the $1+2$D\n",
      "Carrollian conformal algebra hints towards a co-dimension one formalism of flat\n",
      "holography with the field theory residing on the null-boundary of the\n",
      "asymptotically flat space-time enjoying a $1+2$D Carrollian conformal symmetry.\n",
      "Motivated by this fact, we study the general symmetry properties of a\n",
      "source-less $1+2$D Carrollian CFT, adopting a purely field-theoretic approach.\n",
      "After deriving the position-space Ward identities, we show how the $1+3$D bulk\n",
      "super-translation and the super-rotation memory effects emerge from them,\n",
      "manifested by the presence of a temporal step-function factor in the same.\n",
      "Temporal-Fourier transforming these memory effect equations, we directly reach\n",
      "the bulk null-momentum-space leading and sub-leading soft graviton theorems.\n",
      "Along the way, we construct six Carrollian fields $S^\\pm_0$, $S^\\pm_1$, $T$ and\n",
      "$\\bar{T}$ corresponding to these soft graviton fields and the Celestial\n",
      "stress-tensors, purely in terms of the Carrollian stress-tensor components. The\n",
      "2D Celestial shadow-relations and the null-state conditions arise as two\n",
      "natural byproducts of these constructions. We then show that those six fields\n",
      "consist of the modes that implement the super-rotations and a subset of the\n",
      "super-translations on the quantum fields. The temporal step-function allows us\n",
      "to relate the operator product expansions (OPEs) with the operator commutation\n",
      "relations via a complex contour integral prescription. We deduce that not all\n",
      "of those six fields can be taken together to form consistent OPEs. So choosing\n",
      "$S^+_0$, $S^+_1$ and $T$ as the local fields, we form their mutual OPEs using\n",
      "only the OPE-commutativity property, under two general assumptions. The\n",
      "symmetry algebra manifest in these holomorphic-sector OPEs is then shown to be\n",
      "$\\text{Vir}\\ltimes\\hat{\\overline{\\text{sl}(2,\\mathbb{R})}}$ with an abelian\n",
      "ideal.\n",
      "\n",
      "タイトル: 1+3$D フラットホログラフィー本体へのキャロルアプローチ: (拡張された) BMS$_4$ 代数と 1+2$ キャロル共形代数の間の同型は、1+2$ キャロル共形対称性を楽しむ漸近的に平坦な時空のヌル境界に存在する場理論と、平坦なホログラフィーのコ次元1つの形式にヒントを与えている。この事実に動機づけられて、ソースレスの1+2$キャロルCFTの一般的な対称性を研究し、純粋に場理論的なアプローチを採用する。位置空間ウォードアイデンティティを導出した後、1+3$D バルク超変換と超回転メモリ効果がそれらからどのように現れるかを示す。\n",
      "\n",
      "Message posted: 1681619860.629389\n",
      "http://arxiv.org/abs/2304.04476v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-10 09:30:53\n",
      "title: Electromagnetic Interference Cancellation for RIS-Assisted Communications\n",
      "body: Reconfigurable intelligent surface (RIS)-empowered communication is an\n",
      "emerging technology that has recently received growing attention as a potential\n",
      "candidate for next-generation wireless communications. Although RISs have shown\n",
      "the potential of manipulating the wireless channel through passive beamforming,\n",
      "it is shown that they can also bring undesired side effects, such as reflecting\n",
      "the electromagnetic interference (EMI) from the surrounding environment to the\n",
      "receiver side. In this study, we propose a novel EMI cancellation scheme to\n",
      "mitigate the impact of the EMI by exploiting its special time-domain structure\n",
      "and considering a clever passive beamforming method at the RIS. Compared to its\n",
      "benchmark, computer simulations show that the proposed scheme achieves superior\n",
      "performance in terms of the average signal-to-interference-plus-noise ratio\n",
      "(SINR) and outage probability (OP), especially when the EMI power is comparable\n",
      "to the power of the information signal impinging on the RIS surface.\n",
      "\n",
      "再構成可能なインテリジェントサーフェス(RIS)は、パッシブビームフォーミングを通じて無線チャネルを操作する可能性を示しているが、周囲の環境から受信側への電磁干渉(EMI)の反射など、望ましくない副作用をもたらすことも示されている。本研究では、その特別な時間領域構造を活用し、RISにおける巧妙なパッシブビームフォーミング法を考慮し、EMIの影響を軽減するための新しいEMIキャンセルスキームを提案する。\n",
      "\n",
      "Message posted: 1681619939.545209\n",
      "http://arxiv.org/abs/2304.03270v3\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-06 17:55:16\n",
      "title: Fermionic extensions of $W$-algebras via 3d $\\mathcal{N}=4$ gauge theories with a boundary\n",
      "body: We study properties of vertex (operator) algebras associated with 3d\n",
      "H-twisted $\\mathcal{N}=4$ supersymmetric gauge theories with a boundary. The\n",
      "vertex operator algebras (VOAs) are defined by BRST cohomologies of the\n",
      "currents with symplectic bosons, complex fermions, and bc-ghosts. We point out\n",
      "that VOAs for 3d $\\mathcal{N}=4$ abelian gauge theories are fermionic\n",
      "extensions of VOAs associated with toric hyper-K\\\"{a}hler varieties. From this\n",
      "relation, it follows that the VOA associated with the 3d mirror of $N$-flavors\n",
      "$U(1)$ SQED is a fermionic extension of a $W$-algebra\n",
      "$W^{-N+1}(\\mathfrak{sl}_N, f_{\\text{sub}})$. For $N=3$, we explicitly compute\n",
      "the OPEs of elements in the BRST cohomology and find a new algebra which is a\n",
      "fermionic extension of a Bershadsky-Polyakov algebra $W^{-2}(\\mathfrak{sl}_3,\n",
      "f_{\\text{sub}})$. We also suggest an expression of the vacuum character of the\n",
      "fermionic extension of $W^{-N+1}(\\mathfrak{sl}_N, f_{\\text{sub}})$ predicted by\n",
      "3d $\\mathcal{N}=4$ mirror symmetry.\n",
      "\n",
      "境界を持つ3d $\\mathcal{N}=4$ゲージ理論による$W$-代数のフェルミオン拡張は、[arXiv: 1903.08432](https://arxiv.org/abs/1903.08432). 特に、3dミラー対称性に関連する$W^{-N+1}(\\mathfrak{sl}_N,f_{\\text{sub}})$のフェルミオン拡張を研究する。\n",
      "\n",
      "Message posted: 1681619984.652029\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(query_list)):\n",
    "    query = query_list[j]\n",
    "    # arxiv APIで最新の論文情報を取得する\n",
    "    search = arxiv.Search(\n",
    "        query=query,  # 検索クエリ（\n",
    "        max_results=5,  # 取得する論文数\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,  # 論文を投稿された日付でソートする\n",
    "        sort_order=arxiv.SortOrder.Descending,  # 新しい論文から順に取得する\n",
    "    )\n",
    "    \n",
    "    #searchの結果をリストに格納\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_list.append(result)\n",
    "\n",
    "    #ランダムにnum_papersの数だけ選ぶ\n",
    "    num_papers = 5\n",
    "    results = random.sample(result_list, k=num_papers)\n",
    "    \n",
    "    today = time.strftime('%Y-%m-%d', time.localtime())\n",
    "    for i, result in enumerate(results):\n",
    "        print(result)\n",
    "        message_base =  \"本日 \" + str(today) + f\"{message_list[j]} の\" + \"論文 \" + str(i+1) + \"本目です\\n\" + f\"リンク: {result}\\n\"\n",
    "        \n",
    "        text = get_summary(result)\n",
    "        message = message_base + text\n",
    "        try:\n",
    "            # Slackにメッセージを投稿する\n",
    "            response = client.chat_postMessage(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                text=message\n",
    "            )\n",
    "            print(f\"Message posted: {response['ts']}\")\n",
    "        except SlackApiError as e:\n",
    "            print(f\"Error posting message: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7aa8b-9199-4dd6-9571-6ae5bc8426f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
