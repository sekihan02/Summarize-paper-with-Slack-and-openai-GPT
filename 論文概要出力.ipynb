{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8707ecdb-7745-4069-b98a-2058ef7b901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: slack_sdk==3.21.0 in /usr/local/lib/python3.8/dist-packages (3.21.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install slack_sdk==3.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8f9f0d-b5c9-4536-9b2c-64c2ca9653b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv==1.4.4 in /usr/local/lib/python3.8/dist-packages (1.4.4)\n",
      "Requirement already satisfied: feedparser in /usr/local/lib/python3.8/dist-packages (from arxiv==1.4.4) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.8/dist-packages (from feedparser->arxiv==1.4.4) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install arxiv==1.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726c930-7060-42f2-9beb-e0e1d9c104ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc9f605-7fd7-4353-a052-a8c679f259d6",
   "metadata": {},
   "source": [
    "# SlackとopenaiのGPTで論文の要約をする\n",
    "\n",
    "## Reference\n",
    "- [最新の論文をChatGPTで要約して毎朝Slackに共有してくれるbotを作る！](https://zenn.dev/ozushi/articles/ebe3f47bf50a86)\n",
    "- [Slack API を使用してメッセージを投稿する](https://zenn.dev/kou_pg_0131/articles/slack-api-post-message)\n",
    "- [【Slack】インストールするボットユーザーがありませんと出たときの対処方法](https://the-simple.jp/slack-nobotuser#Step1Bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53335cc1-4265-458a-9530-c3330f5654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a128ba8e-74b4-4299-9fb9-c1d3dd3ebe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-04-15 06:02:54.482868: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802033a1-253d-4ac0-8619-c98b0908f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLACK_API_TOKEN = 'SLACK_API_TOKEN'    # ボットとして API を実行するためのトークン\n",
    "# Slackに投稿するチャンネル名を指定する\n",
    "SLACK_CHANNEL = \"要約\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ebe6d8-3518-49cc-9529-616c5f189812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(result):\n",
    "    system = \"\"\"与えられた論文の全体を128文字以内にまとめた後、タイトルと原文とその日本語訳、概要、日付、新規性や差分、独特の手法、実験結果（評価結果の数値がある場合は、この実験結果にその内容を書いてください）を以下のフォーマットで日本語で出力してください。```\n",
    "    # タイトルの原文\n",
    "    # タイトルの日本語訳\n",
    "    # リンク\n",
    "    # 日付(yyyy/MM/dd)\n",
    "    ## 一言でいうと\n",
    "    ### 概要\n",
    "    ### 新規性・差分\n",
    "    ### 手法\n",
    "    ### 結果\n",
    "    ### コメント\n",
    "    ```\"\"\"\n",
    "\n",
    "    # 論文の要約を取得して日本語に翻訳する\n",
    "    summary = result.summary\n",
    "    # 論文のタイトルを取得して日本語に翻訳する\n",
    "    title = result.title\n",
    "    \n",
    "    text = f\"title: {title}\\nbody: {summary}\"\n",
    "    date_str = result.published.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'# 日付(yyyy/MM/dd)\\n{date_str}')\n",
    "    print(text)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Load the MarianMTModel and MarianTokenizer for English to Japanese translation\n",
    "    fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')\n",
    "    \n",
    "    try:\n",
    "        j_text = fugu_translator(text)\n",
    "    except IndexError:\n",
    "        j_text = []\n",
    "        pass\n",
    "    \n",
    "    japanese_translations = []\n",
    "    for translation in j_text:\n",
    "        japanese_translations.append(translation['translation_text'])\n",
    "    \n",
    "    japanese_text = ''.join(japanese_translations)\n",
    "    print(japanese_text)\n",
    "    \n",
    "    text_ = f\"title: {title}\\ndate: {date_str}\\n\"\n",
    "    japan_text = text_ + japanese_text\n",
    "    print()\n",
    "    return japan_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0af268-b7f4-4dfc-9b94-2ac535d31f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryを用意\n",
    "# query_list = ['ti:%22 Anomaly Detection %22', 'ti:%22 AIOps %22']\n",
    "query_list = ['Anomaly Detection', 'AIOps']\n",
    "message_list = ['Anomaly Detection', 'AIOps']\n",
    "\n",
    "# query_list = ['AIOps']\n",
    "# message_list = ['AIOps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a535352-2b11-49d8-ae70-d2093712c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slack APIクライアントを初期化する\n",
    "client = WebClient(token=SLACK_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aafaf00-afb4-4ce4-83cf-868143c3c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2304.06710v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 17:57:54\n",
      "title: Remote Sensing Change Detection With Transformers Trained from Scratch\n",
      "body: Current transformer-based change detection (CD) approaches either employ a\n",
      "pre-trained model trained on large-scale image classification ImageNet dataset\n",
      "or rely on first pre-training on another CD dataset and then fine-tuning on the\n",
      "target benchmark. This current strategy is driven by the fact that transformers\n",
      "typically require a large amount of training data to learn inductive biases,\n",
      "which is insufficient in standard CD datasets due to their small size. We\n",
      "develop an end-to-end CD approach with transformers that is trained from\n",
      "scratch and yet achieves state-of-the-art performance on four public\n",
      "benchmarks. Instead of using conventional self-attention that struggles to\n",
      "capture inductive biases when trained from scratch, our architecture utilizes a\n",
      "shuffled sparse-attention operation that focuses on selected sparse informative\n",
      "regions to capture the inherent characteristics of the CD data. Moreover, we\n",
      "introduce a change-enhanced feature fusion (CEFF) module to fuse the features\n",
      "from input image pairs by performing a per-channel re-weighting. Our CEFF\n",
      "module aids in enhancing the relevant semantic changes while suppressing the\n",
      "noisy ones. Extensive experiments on four CD datasets reveal the merits of the\n",
      "proposed contributions, achieving gains as high as 14.27\\% in\n",
      "intersection-over-union (IoU) score, compared to the best-published results in\n",
      "the literature. Code is available at\n",
      "\\url{https://github.com/mustansarfiaz/ScratchFormer}.\n",
      "\n",
      "タイトル: スクラッチボディからトレーニングされたトランスフォーマーによるリモートセンシング変更検出(Remote Sensing Change Detection With Transformers): 現在のトランスフォーマーベースの変更検出(CD)アプローチは、大規模な画像分類ImageNetデータセットでトレーニングされた事前トレーニングモデルを採用するか、他のCDデータセットで事前トレーニングを行い、ターゲットベンチマークで微調整する。この現在の戦略は、トランスフォーマーが通常、小さなサイズで標準CDデータセットでは不十分な帰納バイアスを学習するために大量のトレーニングデータを必要とするという事実によって駆動される。我々は、スクラッチからトレーニングされたトランスフォーマーによるエンドツーエンドのCDアプローチを開発し、4つの公開ベンチマークで最先端のパフォーマンスを達成している。従来の自己注意を使用してスクラッチからトレーニングされた帰納バイアスをキャプチャする代わりに、我々のアーキテクチャは、選択されたスパース特性のキャプチャに焦点をあてたシャッフルスパースアテンション操作を利用する。\n",
      "\n",
      "Message posted: 1681538589.136889\n",
      "http://arxiv.org/abs/2304.06693v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 17:49:15\n",
      "title: CATS: The Hubble Constant from Standardized TRGB and Type Ia Supernova Measurements\n",
      "body: The Tip of the Red Giant Branch (TRGB) provides a luminous standard candle\n",
      "for constructing distance ladders to measure the Hubble constant. In practice\n",
      "its measurements via edge-detection response (EDR) are complicated by the\n",
      "apparent fuzziness of the tip and the multi-peak landscape of the EDR. As a\n",
      "result, it can be difficult to replicate due to a case-by-case measurement\n",
      "process. Previously we optimized an unsupervised algorithm, Comparative\n",
      "Analysis of TRGBs (CATs), to minimize the variance among multiple halo fields\n",
      "per host without reliance on individualized choices, achieving state-of-the-art\n",
      "$\\sim$ $<$ 0.05 mag distance measures for optimal data. Further, we found an\n",
      "empirical correlation at 5$\\sigma$ confidence in the GHOSTS halo survey between\n",
      "our measurements of the tip and their contrast ratios (ratio of stars 0.5 mag\n",
      "just below and above the tip), useful for standardizing the apparent tips at\n",
      "different host locations. Here, we apply this algorithm to an expanded sample\n",
      "of SN Ia hosts to standardize these to multiple fields in the geometric anchor,\n",
      "NGC 4258. In concert with the Pantheon$+$ SN Ia sample, this analysis produces\n",
      "a (baseline) result of $H_0= 73.22 \\pm 2.06$ km/s/Mpc. The largest difference\n",
      "in $H_0$ between this and similar studies employing the TRGB derives from\n",
      "corrections for SN survey differences and local flows used in most recent SN Ia\n",
      "compilations but which were absent in earlier studies. SN-related differences\n",
      "total $\\sim$ 2.0 km/s/Mpc. A smaller share, $\\sim$ 1.4 km/s/Mpc, results from\n",
      "the inhomogeneity of the TRGB calibration across the distance ladder. We employ\n",
      "a grid of 108 variants around the optimal TRGB algorithm and find the median of\n",
      "variants is $72.94\\pm1.98$ km/s/Mpc with an additional uncertainty due to\n",
      "algorithm choices of 0.83 km/s/Mpc. None of these TRGB variants result in $H_0$\n",
      "less than 71.6 km/s/Mpc.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Your input_length: 563 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Message posted: 1681538592.013159\n",
      "http://arxiv.org/abs/2304.06640v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 16:09:25\n",
      "title: Prospects for detecting anisotropies and polarization of the stochastic gravitational wave background with ground-based detectors\n",
      "body: We build an analytical framework to study the observability of anisotropies\n",
      "and a net chiral polarization of the Stochastic Gravitational Wave Background\n",
      "(SGWB) with a generic network of ground-based detectors. We apply this\n",
      "formalism to perform a Fisher forecast of the performance of a network\n",
      "consisting of the current interferometers (LIGO, Virgo and KAGRA) and planned\n",
      "third-generation ones, such as the Einstein Telescope and Cosmic Explorer. Our\n",
      "results yield limits on the observability of anisotropic modes, spanning across\n",
      "noise- and signal-dominated regimes. We find that if the isotropic component of\n",
      "the SGWB has an amplitude close to the current limit, third-generation\n",
      "interferometers with an observation time of $10$ years can measure multipoles\n",
      "(in a spherical harmonic expansion) up to $\\ell = 8$ with ${\\cal O }\\left(\n",
      "10^{-3} - 10^{-2} \\right)$ accuracy relative to the isotropic component, and an\n",
      "${\\cal O }\\left( 10^{-3} \\right)$ amount of net polarization. For weaker\n",
      "signals, the accuracy worsens as roughly the inverse of the SGWB amplitude.\n",
      "\n",
      "タイトル: 確率重力波背景の異方性および偏光を地上検出器本体で検出する展望: 確率重力波背景(SGWB)の観測可能性と確率重力波背景(SGWB)の正味キラル偏光を研究するための分析フレームワークを構築し、現在の干渉計(LIGO, Virgo, KAGRA)とアインシュタイン望遠鏡や宇宙探査器のような計画された第3世代ネットワークの性能のフィッシャー予測を行うためにこの形式を適用し、ノイズおよび信号支配的なレジームにまたがる異方性モードの観測可能性に制限を与える。\n",
      "\n",
      "Message posted: 1681538602.925079\n",
      "http://arxiv.org/abs/2304.06653v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 16:28:07\n",
      "title: G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection\n",
      "body: It has been reported that clustering-based topic models, which cluster\n",
      "high-quality sentence embeddings with an appropriate word selection method, can\n",
      "generate better topics than generative probabilistic topic models. However,\n",
      "these approaches suffer from the inability to select appropriate parameters and\n",
      "incomplete models that overlook the quantitative relation between words with\n",
      "topics and topics with text. To solve these issues, we propose graph to topic\n",
      "(G2T), a simple but effective framework for topic modelling. The framework is\n",
      "composed of four modules. First, document representation is acquired using\n",
      "pretrained language models. Second, a semantic graph is constructed according\n",
      "to the similarity between document representations. Third, communities in\n",
      "document semantic graphs are identified, and the relationship between topics\n",
      "and documents is quantified accordingly. Fourth, the word--topic distribution\n",
      "is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T\n",
      "achieved state-of-the-art performance on both English and Chinese documents\n",
      "with different lengths. Human judgements demonstrate that G2T can produce\n",
      "topics with better interpretability and coverage than baselines. In addition,\n",
      "G2T can not only determine the topic number automatically but also give the\n",
      "probabilistic distribution of words in topics and topics in documents. Finally,\n",
      "G2T is publicly available, and the distillation experiments provide instruction\n",
      "on how it works.\n",
      "\n",
      "タイトル:G2T: 事前訓練された言語モデルとコミュニティ検出体に基づくトピックモデリングのための単純だが汎用的なフレームワーク: 適切な単語選択手法で高品質な文埋め込みをクラスタリングするクラスタリングベースのトピックモデルが、生成確率論的トピックモデルよりも優れたトピックを生成することが報告されている。しかし、これらのアプローチは、トピックとトピックとトピックとの定量的な関係を見逃す適切なパラメータと不完全なモデルを選択することができない。これらの問題を解決するために、トピックモデリングのための単純だが効果的なフレームワークであるグラフ(G2T)を提案する。フレームワークは4つのモジュールで構成されている。第1に、事前訓練された言語モデルを用いて文書表現が取得される。第2に、文書意味グラフの類似性に従って意味グラフが構築される。第3に、文書の意味グラフ内のコミュニティが識別され、トピックと文書の関係が定量化される。第4に、T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-。\n",
      "\n",
      "Message posted: 1681538633.885019\n",
      "http://arxiv.org/abs/2304.06696v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-13 17:50:15\n",
      "title: Improving novelty detection with generative adversarial networks on hand gesture data\n",
      "body: We propose a novel way of solving the issue of classification of\n",
      "out-of-vocabulary gestures using Artificial Neural Networks (ANNs) trained in\n",
      "the Generative Adversarial Network (GAN) framework. A generative model augments\n",
      "the data set in an online fashion with new samples and stochastic target\n",
      "vectors, while a discriminative model determines the class of the samples. The\n",
      "approach was evaluated on the UC2017 SG and UC2018 DualMyo data sets. The\n",
      "generative models performance was measured with a distance metric between\n",
      "generated and real samples. The discriminative models were evaluated by their\n",
      "accuracy on trained and novel classes. In terms of sample generation quality,\n",
      "the GAN is significantly better than a random distribution (noise) in mean\n",
      "distance, for all classes. In the classification tests, the baseline neural\n",
      "network was not capable of identifying untrained gestures. When the proposed\n",
      "methodology was implemented, we found that there is a trade-off between the\n",
      "detection of trained and untrained gestures, with some trained samples being\n",
      "mistaken as novelty. Nevertheless, a novelty detection accuracy of 95.4% or\n",
      "90.2% (depending on the data set) was achieved with just 5% loss of accuracy on\n",
      "trained classes.\n",
      "\n",
      "タイトル: ハンドジェスチャデータ本体上の生成的敵対ネットワークによる新規性検出の改善: GAN(Generative Adversarial Network)フレームワークでトレーニングされた人工ニューラルネットワーク(ANN)を用いた語彙外ジェスチャ(out-of-vocabulary gestures)の分類の問題を解決する新しい方法を提案する。生成モデルはオンラインの方法で新しいサンプルとターゲットベクターでデータセットを増強し、識別モデルはサンプルのクラスを決定する。UC2017 SGとUC2018 DualMyoデータセットで評価された。生成モデルの性能は、生成されたサンプルと実際のサンプルの間の距離メトリックで測定された。サンプル品質の点でGANは、すべてのクラスの平均的なランダム分布(ノイズ)よりも優れている。分類テストでは、ベースラインニューラルネットワークは、トレーニングされていないジェスチャスチャを識別できなかった。\n",
      "\n",
      "Message posted: 1681538644.827849\n",
      "http://arxiv.org/abs/2304.02829v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-06 02:34:07\n",
      "title: SoK: Machine Learning for Continuous Integration\n",
      "body: Continuous Integration (CI) has become a well-established software\n",
      "development practice for automatically and continuously integrating code\n",
      "changes during software development. An increasing number of Machine Learning\n",
      "(ML) based approaches for automation of CI phases are being reported in the\n",
      "literature. It is timely and relevant to provide a Systemization of Knowledge\n",
      "(SoK) of ML-based approaches for CI phases. This paper reports an SoK of\n",
      "different aspects of the use of ML for CI. Our systematic analysis also\n",
      "highlights the deficiencies of the existing ML-based solutions that can be\n",
      "improved for advancing the state-of-the-art.\n",
      "\n",
      "タイトル: SoK: Machine Learning for Continuous Integration body: Continuous Integration (CI)は、ソフトウェア開発中のコード変更を自動的かつ継続的に統合するための、確立されたソフトウェア開発プラクティスとなっている。文献では、CIフェーズの自動化のための機械学習(ML)ベースのアプローチの増加が報告されている。CIフェーズに対するMLベースのアプローチのシステム化(SoK)を提供することは、タイムリーかつ関連性がある。本稿では、CIに対するMLの使用の異なる側面のSoKについて報告する。また、体系的な分析では、最先端のMLベースのソリューションを改善するために改善できる既存のMLベースのソリューションの不足についても強調する。\n",
      "\n",
      "Message posted: 1681538653.807829\n",
      "http://arxiv.org/abs/2212.13245v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2022-12-26 18:24:45\n",
      "title: Studying the Characteristics of AIOps Projects on GitHub\n",
      "body: Artificial Intelligence for IT Operations (AIOps) leverages AI approaches to\n",
      "handle the massive data generated during the operations of software systems.\n",
      "Prior works have proposed various AIOps solutions to support different tasks in\n",
      "system operations and maintenance (e.g., anomaly detection). In this work, we\n",
      "investigate open-source AIOps projects in-depth to understand the\n",
      "characteristics of AIOps in practice. We first carefully identify a set of\n",
      "AIOps projects from GitHub and analyze their repository metrics (e.g., the used\n",
      "programming languages). Then, we qualitatively study the projects to understand\n",
      "their input data, analysis techniques, and goals. Finally, we analyze the\n",
      "quality of these projects using different quality metrics, such as the number\n",
      "of bugs. We also sample two sets of baseline projects from GitHub: a random\n",
      "sample of machine learning projects, and a random sample of general purpose\n",
      "projects. We compare different metrics of our identified AIOps projects with\n",
      "these baselines. Our results show a recent and growing interest in AIOps\n",
      "solutions. However, the quality metrics indicate that AIOps projects suffer\n",
      "from more issues than our baseline projects. We also pinpoint the most common\n",
      "issues in AIOps approaches and discuss the possible solutions to overcome them.\n",
      "Our findings help practitioners and researchers understand the current state of\n",
      "AIOps practices and sheds light to different ways to improve AIOps weak\n",
      "aspects. To the best of our knowledge, this work is the first to characterize\n",
      "open source AIOps projects.\n",
      "\n",
      "タイトル: GitHubでAIOpsプロジェクトの特性を研究する: IT運用のための人工知能(AIOps)は、ソフトウェアシステムの運用中に生成された大量のデータを処理するためにAIアプローチを活用する。以前の研究は、システム操作とメンテナンス(異常検出など)において、さまざまなタスクをサポートするAIOpsソリューションを提案してきた。この研究では、AIOpsの特性を理解するために、オープンソースのAIOpsプロジェクトを詳細に調査する。まず、GitHubからAIOpsプロジェクトのセットを慎重に識別し、リポジトリメトリクス(使用済みプログラミング言語など)を分析します。その後、入力データ、分析技術、目標を理解するためのプロジェクトを定性的に研究します。最後に、バグの数などの異なる品質指標を使用して、これらのプロジェクトの品質を分析します。また、GitHubからの2つのベースラインプロジェクトをサンプルします。機械学習プロジェクトのランダムなサンプルと、一般的な目的のサンプルです。\n",
      "\n",
      "Message posted: 1681538663.448889\n",
      "http://arxiv.org/abs/2304.04661v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-04-10 15:38:12\n",
      "title: AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges\n",
      "body: Artificial Intelligence for IT operations (AIOps) aims to combine the power\n",
      "of AI with the big data generated by IT Operations processes, particularly in\n",
      "cloud infrastructures, to provide actionable insights with the primary goal of\n",
      "maximizing availability. There are a wide variety of problems to address, and\n",
      "multiple use-cases, where AI capabilities can be leveraged to enhance\n",
      "operational efficiency. Here we provide a review of the AIOps vision, trends\n",
      "challenges and opportunities, specifically focusing on the underlying AI\n",
      "techniques. We discuss in depth the key types of data emitted by IT Operations\n",
      "activities, the scale and challenges in analyzing them, and where they can be\n",
      "helpful. We categorize the key AIOps tasks as - incident detection, failure\n",
      "prediction, root cause analysis and automated actions. We discuss the problem\n",
      "formulation for each task, and then present a taxonomy of techniques to solve\n",
      "these problems. We also identify relatively under explored topics, especially\n",
      "those that could significantly benefit from advances in AI literature. We also\n",
      "provide insights into the trends in this field, and what are the key investment\n",
      "opportunities.\n",
      "\n",
      "タイトル:AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities and Challenges body: Artificial Intelligence for IT Operations (AIOps)は、AIの力を、特にクラウドインフラストラクチャにおいて、IT運用プロセスによって生成されたビッグデータと組み合わせて、可用性を最大化するための行動可能な洞察を提供することを目的としています。AIの能力を運用効率を高めるために活用できるさまざまな問題と、複数のユースケースがあります。ここでは、AIOpsのビジョン、傾向の課題と機会のレビューを紹介します。特に、IT運用活動によって発生するデータの種類、分析の規模と課題、およびそれらがどこに役立つかについて詳しく論じます。重要なAIOpsタスクは、インシデント検出、障害予測、根本原因分析、自動化されたアクションとして分類されます。これらの課題についても、これらの課題を議論し、これらの課題を解決するために、これらの課題について、特に分析し、これらの課題について分析し、また、これらの課題について分析し、また、これらの課題について分析し、また、これらの課題を解決するために、これらの課題について、特に分析し、また、これらの課題について分析し、さらに検討した。\n",
      "\n",
      "Message posted: 1681538679.195169\n",
      "http://arxiv.org/abs/2301.08851v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2023-01-21 01:58:34\n",
      "title: LWS: A Framework for Log-based Workload Simulation in Session-based SUT\n",
      "body: Microservice-based applications and cloud-native systems have been widely\n",
      "applied in large IT enterprises. The operation and management of\n",
      "microservice-based applications and cloud-native systems have become the focus\n",
      "of research. Essential and real workloads are the premise and basis of\n",
      "prominent research topics including performance testing, dynamic resource\n",
      "provisioning and scheduling, and AIOps. Due to the privacy restriction, the\n",
      "complexity and variety of workloads, and the requirements for reasonable\n",
      "intervention, it is difficult to copy or generate real workloads directly. In\n",
      "this paper, we formulate the task of workload simulation and propose a\n",
      "framework for Log-based Workload Simulation (LWS) in session-based application\n",
      "systems. First, LWS collects session logs and transforms them into grouped and\n",
      "well-organized sessions. Then LWS extracts the user behavior abstraction based\n",
      "on a relational model and the intervenable workload intensity by three methods\n",
      "from different perspectives. LWS combines the user behavior abstraction and the\n",
      "workload intensity for simulated workload generation and designs a\n",
      "domain-specific language for better execution. The experimental evaluation is\n",
      "performed on an open-source cloud-native application and a public real-world\n",
      "e-commerce workload. The experimental results show that the simulated workload\n",
      "generated by LWS is effective and intervenable.\n",
      "\n",
      "タイトル: LWS: A Framework for Log-based Workload Simulation in Session-based SUT body: Microservice-based applications and cloud-native systemsは、大規模IT企業に広く適用されている。マイクロサービスベースのアプリケーションとクラウドネイティブシステムの運用と管理は、研究の焦点となっている。本質的かつ実際のワークロードは、パフォーマンステスト、動的リソースプロビジョニング、スケジューリング、AIOpsなどの著名な研究トピックの前提と基礎である。プライバシー制限、複雑さ、さまざまなワークロード、および合理的な介入の要件により、実際のワークロードを直接コピーまたは生成することは困難である。本稿では、ワークロードシミュレーションのタスクを定式化し、セッションベースのアプリケーションシステムにおけるログベースのワークロードシミュレーション(LWS)のフレームワークを提案する。まず、LWSはセッションログを収集し、それらをグループ化およびよく組織化されたセッションに変換する。LWSは、LWSの振る舞いと実際のワークロードを3つの異なる方法で抽出する。\n",
      "\n",
      "Message posted: 1681538691.679269\n",
      "http://arxiv.org/abs/2211.15739v1\n",
      "# 日付(yyyy/MM/dd)\n",
      "2022-11-28 19:41:56\n",
      "title: CWD: A Machine Learning based Approach to Detect Unknown Cloud Workloads\n",
      "body: Workloads in modern cloud data centers are becoming increasingly complex. The\n",
      "number of workloads running in cloud data centers has been growing\n",
      "exponentially for the last few years, and cloud service providers (CSP) have\n",
      "been supporting on-demand services in real-time. Realizing the growing\n",
      "complexity of cloud environment and cloud workloads, hardware vendors such as\n",
      "Intel and AMD are increasingly introducing cloud-specific workload acceleration\n",
      "features in their CPU platforms. These features are typically targeted towards\n",
      "popular and commonly-used cloud workloads. Nonetheless, uncommon,\n",
      "customer-specific workloads (unknown workloads), if their characteristics are\n",
      "different from common workloads (known workloads), may not realize the\n",
      "potential of the underlying platform. To address this problem of realizing the\n",
      "full potential of the underlying platform, we develop a machine learning based\n",
      "technique to characterize, profile and predict workloads running in the cloud\n",
      "environment. Experimental evaluation of our technique demonstrates good\n",
      "prediction performance. We also develop techniques to analyze the performance\n",
      "of the model in a standalone manner.\n",
      "\n",
      "タイトル: CWD: 未知のクラウドワークロードを検出するための機械学習ベースのアプローチ: 現代のクラウドデータセンタのワークロードはますます複雑化しています。過去数年間、クラウドデータセンターで実行されるワークロードの数は指数関数的に増加しており、クラウドサービスプロバイダ(CSP)はオンデマンドサービスをリアルタイムでサポートしています。クラウド環境やクラウドワークロードの複雑さの増大を実現するため、IntelやAMDなどのハードウェアベンダは、CPUプラットフォームにクラウド固有のワークロードアクセラレーション機能を導入しています。これらの機能は、一般的には一般的なクラウドワークロードを対象としていますが、一般的なワークロード(既知のワークロード)とは異なる場合、その特性は基盤となるプラットフォームの可能性に気付かないかもしれません。我々は、機械学習ベースのテクニックの全体的可能性を実現するために、機械学習ベースのパフォーマンス予測、予測、予測、予測、予測、予測、予測、予測、予測、予測、予測、予測、予測などの方法も提供しています。\n",
      "\n",
      "Message posted: 1681538703.897979\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(query_list)):\n",
    "    query = query_list[j]\n",
    "    # arxiv APIで最新の論文情報を取得する\n",
    "    search = arxiv.Search(\n",
    "        query=query,  # 検索クエリ（\n",
    "        max_results=5,  # 取得する論文数\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,  # 論文を投稿された日付でソートする\n",
    "        sort_order=arxiv.SortOrder.Descending,  # 新しい論文から順に取得する\n",
    "    )\n",
    "    \n",
    "    #searchの結果をリストに格納\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_list.append(result)\n",
    "\n",
    "    #ランダムにnum_papersの数だけ選ぶ\n",
    "    num_papers = 5\n",
    "    results = random.sample(result_list, k=num_papers)\n",
    "    \n",
    "    today = time.strftime('%Y-%m-%d', time.localtime())\n",
    "    for i, result in enumerate(results):\n",
    "        print(result)\n",
    "        message_base =  \"本日 \" + str(today) + f\"{message_list[j]} の\" + \"論文 \" + str(i+1) + \"本目です\\n\" + f\"リンク: {result}\\n\"\n",
    "        \n",
    "        text = get_summary(result)\n",
    "        message = message_base + text\n",
    "        try:\n",
    "            # Slackにメッセージを投稿する\n",
    "            response = client.chat_postMessage(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                text=message\n",
    "            )\n",
    "            print(f\"Message posted: {response['ts']}\")\n",
    "        except SlackApiError as e:\n",
    "            print(f\"Error posting message: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7aa8b-9199-4dd6-9571-6ae5bc8426f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7740b-96e4-42ae-9dda-1967e21dcaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
