{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4ade81-4df8-4ff5-8f0b-abcd78d35180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.6\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a21819-6ef7-48a2-8e47-500c7999dc32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.152 in /usr/local/lib/python3.10/dist-packages (0.0.152)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>1.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (2.0.12)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (1.23.5)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (2.29.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (8.2.2)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.152) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.152) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.152) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.152) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.152) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.152) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.152) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.152) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.152) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.152) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.152) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.152) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.152) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.152) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>1.3->langchain==0.0.152) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.152) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.152) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: slack_sdk==3.21.0 in /usr/local/lib/python3.10/dist-packages (3.21.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: arxiv==1.4.4 in /usr/local/lib/python3.10/dist-packages (1.4.4)\n",
      "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from arxiv==1.4.4) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->arxiv==1.4.4) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting flash-attn==1.0.3.post0\n",
      "  Using cached flash_attn-1.0.3.post0.tar.gz (2.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[21 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-75ln7mu4/flash-attn_0dfb49ad5087444c99c64eef76f1e3f7/setup.py\", line 109, in <module>\n",
      "  \u001b[31m   \u001b[0m     _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-75ln7mu4/flash-attn_0dfb49ad5087444c99c64eef76f1e3f7/setup.py\", line 24, in get_cuda_bare_metal_version\n",
      "  \u001b[31m   \u001b[0m     raw_output = subprocess.check_output([cuda_dir + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 420, in check_output\n",
      "  \u001b[31m   \u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 501, in run\n",
      "  \u001b[31m   \u001b[0m     with Popen(*popenargs, **kwargs) as process:\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 969, in __init__\n",
      "  \u001b[31m   \u001b[0m     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.10/subprocess.py\", line 1845, in _execute_child\n",
      "  \u001b[31m   \u001b[0m     raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "  \u001b[31m   \u001b[0m FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/cuda/bin/nvcc'\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.0.0+cu117\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.0.152\n",
    "!pip3 install slack_sdk==3.21.0\n",
    "!pip3 install arxiv==1.4.4\n",
    "!pip install wikipedia\n",
    "!pip install einops\n",
    "!pip install flash-attn==1.0.3.post0 triton==2.0.0.dev20221202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549cdcf5-c4ce-4b04-b7b6-209322c7facc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdddc87e-bc28-4368-a197-72bd79368cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"処理時間を表示するクラス\n",
    "    with Timer(prefix=f'pred cv={i}'):\n",
    "        y_pred_i = predict(model, loader=test_loader)\n",
    "    \n",
    "    with Timer(prefix='fit fold={} '.format(i)):\n",
    "        clf.fit(x_train, y_train, \n",
    "                eval_set=[(x_valid, y_valid)],  \n",
    "                early_stopping_rounds=100,\n",
    "                verbose=verbose)\n",
    "\n",
    "    with Timer(prefix='fit fold={} '.format(i), verbose=500):\n",
    "        clf.fit(x_train, y_train, \n",
    "                eval_set=[(x_valid, y_valid)],  \n",
    "                early_stopping_rounds=100,\n",
    "                verbose=verbose)\n",
    "    \"\"\"\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72c4061-3f9e-4476-a2ad-2dc9c04f8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b145d2-7074-489b-92d6-a91c0219a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc22f18d3084da1b2a9aae4d79eb4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5859e54da04598a9e3564ba258f779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64b7eeb479044acadc9a5338ac9b73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-chat/fa9345560420e2111498f87799ea7dd31cbd647e/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ba0a1efd09495fb5d0cb4fc8bddef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4276f48b3a124b51aed3a401f09c4f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MPTForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set model 15214.194[s]\n",
      "CPU times: user 6min 56s, sys: 1min 46s, total: 8min 42s\n",
      "Wall time: 4h 13min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# # LLMの準備\n",
    "# llm = OpenAI(temperature=0.9)\n",
    "\n",
    "import torch\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline\n",
    "\n",
    "device_num = -1 # CPU\n",
    "if torch.cuda.is_available():\n",
    "    device_num = 0 # cuda:0\n",
    "\n",
    "with Timer(prefix=f'set model'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mosaicml/mpt-7b-chat\"\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mosaicml/mpt-7b-chat\", \n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    ).to(\"cuda:0\")\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", model=model, tokenizer=tokenizer,\n",
    "      max_new_tokens=256, device=device_num, torch_dtype=torch.float16\n",
    "    )\n",
    "    # LLMの準備\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa84911a-aa4c-4a5d-bc82-fd88da9c3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3a706-9369-4c41-ac6f-8df5e12edf24",
   "metadata": {},
   "source": [
    "### SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb673b9-9c84-48c3-8b0b-91cd95bffc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# 1つ目のチェーンの準備 : タイトルからあらすじを生成\n",
    "# template = \"\"\"Please summarize the entire given paper in 128 characters or less, and then output the title, original text and its Japanese translation, summary, novelty or difference, unique methods, and experimental results (if there are numerical evaluation results, please write them in this experimental results) in Japanese in the following format.\n",
    "# ---\n",
    "# Output Example 1\n",
    "# Today, 2023-04-11, is the first paper in the AIOps\n",
    "# Towards a consistent interpretation of AIOps models\n",
    "# ## Towards a consistent interpretation of AIOps models\n",
    "# ## In a nutshell.\n",
    "# Consistency is needed in the interpretation of AIOps models. In this study, we investigate three aspects of consistency in the interpretation of AIOps models: internal consistency, external consistency, and temporal consistency, for two tasks: failure prediction of Google cluster jobs and failure prediction of Backblaze hard drives. Randomness, hyperparameter adjustment, and data sampling control were found to affect interpretation consistency; AIOps models with AUCs greater than 0.75 provide more consistent interpretations than lower performing models. We also found that AIOps models built with a sliding window or full history approach provide the most consistent interpretation with trends across the data set.\n",
    "# ### Overview.\n",
    "# AIOps has been employed by organizations for a variety of tasks, including interpretation of models to identify indicators of service failure. However, many AIOps studies violate established practices in the machine learning community when deriving model interpretations. This study investigates the consistency of AIOps model interpretations and provides guidelines for improving consistency.\n",
    "# ### Novelty/Differences\n",
    "# The novelty of this study is that it investigates the consistency of interpretation of AIOps models from three aspects: internal consistency, external consistency, and temporal consistency, and provides guidelines to improve the consistency of interpretation.\n",
    "# ### Methodology\n",
    "# We investigated the consistency of interpretation of the AIOps model for two tasks: failure prediction of Google cluster jobs and failure prediction of Backblaze hard drives. We investigated three aspects: internal consistency, external consistency, and temporal consistency, and showed that randomness, hyperparameter adjustment, and data sampling control affect interpretation consistency. We also found that AIOps models with AUCs greater than 0.75 provide more consistent interpretations than lower-performing models. Finally, we found that AIOps models built with a sliding window or full history approach provided the most consistent interpretation of trends across the data set.\n",
    "# ### Results.\n",
    "# We found that factors affecting the consistency of interpretation of AIOps models include randomness, hyperparameter adjustment, and data sampling control; we found that AIOps models with AUCs greater than 0.75 provide more consistent interpretation than lower performing models. We also found that AIOps models built with a sliding window or full history approach provided the most consistent interpretation of trends across the data set.\n",
    "# ### Comments.\n",
    "# This study investigates three aspects of interpretation consistency in AIOps models: internal consistency, external consistency, and temporal consistency, and provides guidelines for improving interpretation consistency. This will help practitioners avoid misunderstandings in interpreting the AIOps model.\n",
    "# ---\n",
    "# Response Format\n",
    "# text:{text}\n",
    "# ---\n",
    "# # Japanese translation of title\n",
    "# ## In a word.\n",
    "# ### Summary\n",
    "# ### Novelty/Difference\n",
    "# ### Methodology\n",
    "# ### Result\n",
    "# ### Comments\n",
    "# ```\"\"\"\n",
    "\n",
    "template = \"\"\"与えられた論文の全体を128文字以内にまとめた後、タイトルと原文とその日本語訳、概要、新規性や差分、独特の手法、実験結果（評価結果の数値がある場合は、この実験結果にその内容を書いてください）を以下のフォーマットで日本語で出力してください。```\n",
    "---\n",
    "Output Example 1\n",
    "Decentralized Federated Learning Preserves Model and Data Privacy\n",
    "分散型連合学習がモデルとデータのプライバシーを保証する\n",
    "リンク: http://arxiv.org/abs/2102.00880v1\n",
    "日付: 2021-02-01 14:38:54\n",
    "ITシステムの機能不全時にサポートするソリューションを求める需要が増えています。\n",
    "概要\n",
    "AIOpsは、学術界と産業界において、ますます重視される研究分野ですが、十分なラベル付けされたデータにアクセスすることが困難な問題があります。この問題を解決する方法として、フェデレーテッド・ラーニングが提供されます。この方法では、トレーニングデータへの直接アクセスが必要ないというメリットがあります。しかし、この方法には、中央インスタンスを介してモデルのパラメータを同期させる必要があります。この中央インスタンスが信頼できる必要があり、また、単一障害点になる可能性もあります。そこで、本研究では、フェデレーテッド・ラーニングのフルに分散型アプローチを提案します。このアプローチでは、トレーニング済みモデル間での知識共有が可能です。トレーニングデータやモデルパラメータの送信は必要ありません。代わりに、教師と生徒の役割がモデルに割り当てられ、生徒は教師の出力を元にトレーニングされます。本研究では、ログ異常検知に関する事例研究を実施しま\n",
    "一言でいうと\n",
    "分散システムにおいて、異常検知を行うために利用できる多源データを利用する方法を提案しています。\n",
    "概要\n",
    "分散システムから生成される多源データを統合的に解析することで、システムの状態の異常を検知することができます。本研究では、分散システムのトレースデータとログデータを統合することで異常検知を行う手法を提案しています。実験により、単一モデリティに基づいた異常検知方法よりも、トレースデータとログデータの統合に基づいた方法の方がより良い結果を生み出すことが示されています。\n",
    "新規性・差分\n",
    "以前の研究では、単一モデリティに基づいた異常検知方法が提案されていましたが、本研究ではトレースデータとログデータの統合に基づいた異常検知方法を提案しています。\n",
    "手法\n",
    "本研究では、トレースデータとログデータを統合することで異常検知を行う手法を提案しています。\n",
    "結果\n",
    "実験により、単一モデリティに基づいた異常検知方法よりも、トレースデータとログデータの統合に基づいた方法の方がより良い結果を生み出すことが示されています。\n",
    "コメント\n",
    "分散システムにおいて、多源データを統合的に利用することで、より効果的な異常検知が行えることが示されています。\n",
    "---\n",
    "Response Format\n",
    "text:{text}\n",
    "---\n",
    "# タイトルの日本語訳\n",
    "## 一言でいうと\n",
    "---\n",
    "### 概要\n",
    "### 新規性・差分\n",
    "### 手法\n",
    "### 結果\n",
    "### コメント\n",
    "```\"\"\"\n",
    "\n",
    "# プロンプトテンプレートの生成\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "# LLMChainの準備\n",
    "result_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    output_key=\"result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a5b115-f58f-4df4-8f8a-4f5d2717ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つ目のチェーン : あらすじの発展生成\n",
    "template = \"\"\"Describe the following sentence in three words.{result}\n",
    "```\n",
    "Word 1:\n",
    "Word 2:\n",
    "Word 3:\n",
    "```\"\"\"\n",
    "\n",
    "# プロンプトテンプレートの準備\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"result\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "# LLMChainの準備\n",
    "word_chain = LLMChain(\n",
    "    llm=llm, prompt=prompt_template, \n",
    "    output_key=\"word\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e709242-ddd3-4189-acce-ca65457b72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5つ目のチェーン : 道具を英訳する\n",
    "# template = \"\"\"あなたは翻訳家です。与えられた文章を日本語に訳してください```\n",
    "# 文章:{result}\n",
    "# 翻訳結果:```\"\"\"\n",
    "\n",
    "# # プロンプトテンプレートの準備\n",
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"result\"], \n",
    "#     template=template\n",
    "# )\n",
    "\n",
    "# # LLMChainの準備\n",
    "# translation_chain = LLMChain(\n",
    "#     llm=llm, prompt=prompt_template, \n",
    "#     output_key=\"translation\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f761b6dc-9ef2-40aa-96db-c8ecd351f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SequentialChainで2つのチェーンを繋ぐ\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[result_chain, word_chain],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"result\", \"word\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0128a-85e5-4cc0-9bd1-25535d14a77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199a89f4-10f2-4c2c-b641-a37cd2d09b34",
   "metadata": {},
   "source": [
    "# SlackとopenaiのGPTで論文の要約をする\n",
    "\n",
    "## Reference\n",
    "- [最新の論文をChatGPTで要約して毎朝Slackに共有してくれるbotを作る！](https://zenn.dev/ozushi/articles/ebe3f47bf50a86)\n",
    "- [Slack API を使用してメッセージを投稿する](https://zenn.dev/kou_pg_0131/articles/slack-api-post-message)\n",
    "- [【Slack】インストールするボットユーザーがありませんと出たときの対処方法](https://the-simple.jp/slack-nobotuser#Step1Bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53335cc1-4265-458a-9530-c3330f5654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22e06e29-7297-48bb-9d62-4da8d6ebb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slack APIトークン\n",
    "\n",
    "SLACK_API_TOKEN = 'SLACK_API_TOKEN'    # ユーザーとして API を実行するためのトークン \n",
    "# Slackに投稿するチャンネル名を指定する\n",
    "SLACK_CHANNEL = \"SLACK_CHANNEL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd932567-4ab5-483e-802b-b09c8a34f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slack APIクライアントを初期化する\n",
    "client = WebClient(token=SLACK_API_TOKEN)\n",
    "#queryを用意\n",
    "# query_list = ['ti:%22 Anomaly Detection %22', 'ti:%22 AIOps %22']\n",
    "query_list = ['Anomaly Detection', 'AIOps']\n",
    "message_list = ['Anomaly Detection', 'AIOps']\n",
    "\n",
    "# query_list = ['AIOps']\n",
    "# message_list = ['AIOps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6987878c-4920-4a48-9084-9e1e487e04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bbfb342-a391-4bc6-a23f-b8390bc810a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece pysbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feab0fc7-1520-4721-9fbc-26059f6b5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MarianMTModel and MarianTokenizer for English to Japanese translation\n",
    "# fugu_translator = pipeline('translation', model='staka/fugumt-en-ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c433ca62-2473-45c4-9448-e98054ada822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2305.02727v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683430135.157659\n",
      "http://arxiv.org/abs/2305.02490v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683430331.113999\n",
      "http://arxiv.org/abs/2305.03024v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683430505.260289\n",
      "http://arxiv.org/abs/2305.02775v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683430685.819559\n",
      "http://arxiv.org/abs/2305.02559v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683430858.039989\n",
      "http://arxiv.org/abs/2208.03938v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683431026.902229\n",
      "http://arxiv.org/abs/2105.04547v2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683431195.838739\n",
      "http://arxiv.org/abs/2209.09645v1\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683431359.390479\n",
      "http://arxiv.org/abs/2206.15033v2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683431533.530109\n",
      "http://arxiv.org/abs/2104.15052v2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Message posted: 1683431700.302179\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(query_list)):\n",
    "    query = query_list[j]\n",
    "    # arxiv APIで最新の論文情報を取得する\n",
    "    search = arxiv.Search(\n",
    "        query=query,  # 検索クエリ（\n",
    "        max_results=50,  # 取得する論文数\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,  # 論文を投稿された日付でソートする\n",
    "        sort_order=arxiv.SortOrder.Descending,  # 新しい論文から順に取得する\n",
    "    )\n",
    "    \n",
    "    #searchの結果をリストに格納\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_list.append(result)\n",
    "\n",
    "    #ランダムにnum_papersの数だけ選ぶ\n",
    "    num_papers = 5\n",
    "    results = random.sample(result_list, k=num_papers)\n",
    "    \n",
    "    today = time.strftime('%Y-%m-%d', time.localtime())\n",
    "    # 論文情報をSlackに投稿する\n",
    "    for i, result in enumerate(results):\n",
    "        gc.collect()\n",
    "        print(result)\n",
    "        try:\n",
    "            # SequentialChainの実行\n",
    "            # Slackに投稿するメッセージを組み立てる\n",
    "            message = overall_chain({\"text\":result.summary})\n",
    "            # 翻訳\n",
    "            # j_text = fugu_translator(message[\"result\"])\n",
    "            # print(j_text)\n",
    "            # print(j_text[\"translation_text\"])\n",
    "            # Slackにメッセージを投稿する\n",
    "            response = client.chat_postMessage(\n",
    "                channel=SLACK_CHANNEL,\n",
    "                # text=\"本日 \" + str(today) + f\"{message_list[j]} の\" + \"論文\\n\" + f\"title: {result.title}\\n\"+f\"link: {result}\\n\"+str(j_text)\n",
    "                text=\"本日 \" + str(today) + f\"{message_list[j]} の\" + \"論文\\n\" + f\"title: {result.title}\\n\"+f\"link: {result}\\n\"+message[\"result\"]\n",
    "            )\n",
    "            print(f\"Message posted: {response['ts']}\")\n",
    "        except SlackApiError as e:\n",
    "            print(f\"Error posting message: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7580670-ba45-450f-aae1-5f99a71c717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Example 2\n",
      "A Novel Deep Learning Framework for Anomaly Detection in Distributed Systems\n",
      "分散システムの異常検知に対するニューラルネットワークのフレームワークの提案\n",
      "リンク: http://arxiv.org/abs/2103.01005v1\n",
      "日付: 2021-03-01 14:36:01\n",
      "ITシステムの機能不全時にサポートするソリューションを求める需要が増えています。\n",
      "概要\n",
      "分散システムの異常検知に対するニューラルネットワークのフレームワークを提案します。\n",
      "このフレームワークは、異常検知のためのニューラルネットワークを提供し、分散システムの異常検知に対する高い精度を提供します。\n",
      "このフレー�\n"
     ]
    }
   ],
   "source": [
    "print(message[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d094319-5614-4d39-afb0-5de0136b55f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "A Novel Deep Learning Framework for Anomaly Detection in Distributed Systems\n",
      "分散システムの異常検知に対するニューラルネットワークのフレームワークの提案\n",
      "リンク: http://arxiv.org/abs/2103.01005v1\n",
      "日付: 2021-03-01 14:36:01\n",
      "ITシステムの機能不全時にサポートするソリューションを求める需要が増えています。\n",
      "概要\n",
      "分散システムの異常検知に対するニューラルネットワークのフレームワークを提案します。\n",
      "このフレームワークは、異常検知のためのニューラルネットワークを提供し、分散システムの異常検知に対する高い精度を提供します。\n",
      "このフレーズ\n"
     ]
    }
   ],
   "source": [
    "print(message[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892e0e7-393d-4ab1-85c6-ecf409bb2243",
   "metadata": {},
   "source": [
    "## ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d06bc006-7354-447d-90b8-3638f265ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "# 数学的処理を実行する必要があるときに利用するツールの構築の下準備\n",
    "# from langchain import LLMMathChain\n",
    "# llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description=\"useful for when you need to ask with search\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description=\"useful for when you need to ask with lookup\"\n",
    "    ),\n",
    "    # # 数学に関する質問に答える必要があるときに利用するツール\n",
    "    # Tool(\n",
    "    #     name=\"Calculator\",\n",
    "    #     func=llm_math_chain.run,\n",
    "    #     description=\"useful for when you need to answer questions about math\"\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccc4f2df-c7b2-45be-a4a4-2a71f74e3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "react = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ba0fb49-7e21-49ed-8514-b4760da905b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'このフレーズ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trans_review = message[\"word\"].split('\\n')[1]\n",
    "trans_review = message[\"word\"].split('\\n')[-1]\n",
    "\n",
    "trans_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6036519-b863-4bef-b60a-a816b3b03de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Output Example 2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message[\"result\"].split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9394f8be-cccf-4564-ba7b-54b6af5ab842",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_review = message[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4617ea4a-d603-4c02-848e-db1daa7562c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del llm; del message; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48794ce8-b47f-4f84-a92b-175819587213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb444b-6447-4ef5-b2bd-5fa71cbfbe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "react_res = react.run(trans_review)\n",
    "# react_res = react.run(\"What is \" + message[\"result\"].split('\\n')[1] + \"?\")\n",
    "\n",
    "# while react_res == 'Agent stopped due to iteration limit or time limit.':\n",
    "#     print('retry')\n",
    "#     react_res = react.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe18f98-88b3-4ece-b3f7-b792c5534a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def384c-0ef5-4425-ab5a-94ac06a48f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
